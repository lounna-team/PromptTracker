# Refactoring Plan: Tests vs Production Monitoring

## üìã Executive Summary

This document outlines the architectural changes needed to properly separate:
- **Tests** (pre-deployment validation)
- **Production Monitoring** (runtime evaluation via `track_llm_call`)

## üéØ Goals

1. **Move EvaluatorConfig from Prompt ‚Üí PromptVersion**
   - Each version has its own evaluation strategy
   - Enables version-specific evaluation criteria
   - Supports A/B testing of evaluation configs

2. **Prevent Auto-Evaluation on Test Runs**
   - Tests control their own evaluators
   - No duplicate evaluations
   - Clear separation of concerns

3. **Distinguish Evaluation Contexts**
   - Production monitoring vs test validation
   - Different UI sections
   - Separate analytics

4. **Unify EvaluatorConfig Model**
   - Remove JSONB `evaluator_configs` from PromptTest
   - Use polymorphic EvaluatorConfig for both PromptVersion and PromptTest
   - Single source of truth

## üìä Current State vs Target State

### Current Architecture

```
Prompt (1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> (N) EvaluatorConfig
  ‚îÇ                      ‚îú‚îÄ evaluator_key
  ‚îÇ                      ‚îú‚îÄ weight
  ‚îÇ                      ‚îú‚îÄ run_mode
  ‚îÇ                      ‚îî‚îÄ config (JSONB)
  ‚îÇ
  ‚îî‚îÄ‚îÄ> (N) PromptVersion
         ‚îî‚îÄ‚îÄ> (N) LlmResponse
                ‚îú‚îÄ after_create :trigger_auto_evaluation (ALWAYS)
                ‚îî‚îÄ‚îÄ> (N) Evaluation

PromptTest
  ‚îú‚îÄ evaluator_configs (JSONB array)
  ‚îÇ    ‚îú‚îÄ evaluator_key
  ‚îÇ    ‚îú‚îÄ threshold
  ‚îÇ    ‚îî‚îÄ config
  ‚îî‚îÄ‚îÄ> (N) PromptTestRun
         ‚îî‚îÄ‚îÄ> LlmResponse (triggers auto-eval + test evals = DUPLICATE!)
```

**Problems:**
- ‚ùå EvaluatorConfig on Prompt (not version-specific)
- ‚ùå Duplicate evaluator config schemas (model vs JSONB)
- ‚ùå Auto-evaluation runs on ALL LlmResponse creation (including tests)
- ‚ùå No distinction between test evals and production evals

### Target Architecture

```
PromptVersion (1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> (N) EvaluatorConfig (polymorphic)
  ‚îÇ                              ‚îú‚îÄ configurable_type: "PromptVersion"
  ‚îÇ                              ‚îú‚îÄ configurable_id
  ‚îÇ                              ‚îú‚îÄ evaluator_key
  ‚îÇ                              ‚îú‚îÄ weight
  ‚îÇ                              ‚îú‚îÄ threshold (NEW)
  ‚îÇ                              ‚îú‚îÄ run_mode
  ‚îÇ                              ‚îú‚îÄ depends_on
  ‚îÇ                              ‚îî‚îÄ config (JSONB)
  ‚îÇ
  ‚îî‚îÄ‚îÄ> (N) LlmResponse
         ‚îú‚îÄ is_test_run (boolean)
         ‚îú‚îÄ after_create :trigger_auto_evaluation, unless: :is_test_run?
         ‚îî‚îÄ‚îÄ> (N) Evaluation
                ‚îî‚îÄ evaluation_context (enum: production_monitoring, test_run, manual)

PromptTest (1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> (N) EvaluatorConfig (polymorphic)
  ‚îÇ                          ‚îú‚îÄ configurable_type: "PromptTest"
  ‚îÇ                          ‚îú‚îÄ configurable_id
  ‚îÇ                          ‚îú‚îÄ evaluator_key
  ‚îÇ                          ‚îú‚îÄ threshold
  ‚îÇ                          ‚îî‚îÄ config (JSONB)
  ‚îÇ
  ‚îî‚îÄ‚îÄ> (N) PromptTestRun
         ‚îî‚îÄ‚îÄ> LlmResponse (is_test_run: true, no auto-eval)
```

**Benefits:**
- ‚úÖ Version-specific evaluation configs
- ‚úÖ Single EvaluatorConfig model (no duplication)
- ‚úÖ Tests don't trigger auto-evaluation
- ‚úÖ Clear context tracking (production vs test)
- ‚úÖ Can copy configs between versions and tests
- ‚úÖ Tests can use dependencies, weights, priorities
- ‚úÖ Production can use thresholds for alerting

## üóÇÔ∏è Implementation Phases

### Phase 1: Database Schema Changes
**Files:** `db/migrate/`, models
**Estimated Time:** 2-3 hours
**Details:** See `REFACTORING_PHASE_1_DATABASE.md`

### Phase 2: Model Updates
**Files:** `app/models/prompt_tracker/`
**Estimated Time:** 3-4 hours
**Details:** See `REFACTORING_PHASE_2_MODELS.md`

### Phase 3: Service Layer Updates
**Files:** `app/services/prompt_tracker/`, `app/jobs/prompt_tracker/`
**Estimated Time:** 2-3 hours
**Details:** See `REFACTORING_PHASE_3_SERVICES.md`

### Phase 4: UI Restructuring
**Files:** `app/controllers/`, `app/views/`, `config/routes.rb`
**Estimated Time:** 4-5 hours
**Details:** See `REFACTORING_PHASE_4_UI.md`

### Phase 5: Testing
**Files:** `spec/`
**Estimated Time:** 4-5 hours
**Details:** See `REFACTORING_PHASE_5_TESTING.md`

## üìÖ Timeline

**Total Estimated Time:** 15-20 hours

**Recommended Approach:**
1. Create feature branch: `refactor/tests-vs-monitoring`
2. Implement phases sequentially (each phase builds on previous)
3. Run tests after each phase
4. Create PR with comprehensive documentation
5. Deploy with data migration plan

## üö® Risk Assessment

### High Risk
- **Data Migration:** Moving EvaluatorConfig from Prompt to PromptVersion
  - Mitigation: Reversible migration, backup data, test thoroughly

### Medium Risk
- **Breaking Changes:** Existing code using `prompt.evaluator_configs`
  - Mitigation: Comprehensive search and replace, deprecation warnings

### Low Risk
- **UI Changes:** New monitoring section
  - Mitigation: Incremental rollout, feature flags

## üìù Success Criteria

- [ ] All tests pass (models, services, controllers)
- [ ] Data migration completes successfully
- [ ] UI clearly separates Tests and Monitoring
- [ ] No duplicate evaluations on test runs
- [ ] Can copy evaluator configs between versions and tests
- [ ] Production monitoring works as expected
- [ ] Documentation updated

## üîÑ Rollback Plan

1. Revert database migrations (down migrations provided)
2. Restore from backup if needed
3. Revert code changes via git
4. Clear cache and restart services

## üìö Related Documents

- `REFACTORING_PHASE_1_DATABASE.md` - Database schema changes
- `REFACTORING_PHASE_2_MODELS.md` - Model updates
- `REFACTORING_PHASE_3_SERVICES.md` - Service layer changes
- `REFACTORING_PHASE_4_UI.md` - UI restructuring
- `REFACTORING_PHASE_5_TESTING.md` - Testing strategy

