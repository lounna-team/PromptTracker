# Testing Guide for PromptTracker

## Overview

PromptTracker uses **two test frameworks** to ensure comprehensive coverage:

- **Minitest** - Original test suite (14 files, ~412 tests)
- **RSpec** - Comprehensive test suite (19 files, ~271 tests)
  - Business Logic: ~100 tests (Models, Services, Registry)
  - Controllers: 144 tests (All 7 controllers)
  - Jobs: 27 tests (Both background jobs)

**Total Coverage:** ~683 tests across critical functionality

**Code Coverage:** 89.64% line coverage, 69.64% branch coverage (via SimpleCov)

---

## ğŸš€ Quick Start

### Run All Tests (Recommended)

```bash
# Option 1: Using the custom script (with colored output)
./bin/test_all

# Option 2: Using Rake task
bundle exec rake test_all

# Option 3: Using the default rake task
bundle exec rake
```

### Run Individual Test Suites

```bash
# Run only Minitest
bundle exec rails test

# Run only RSpec
bundle exec rspec
```

---

## ğŸ“ Test Structure

### Minitest Tests (`test/` directory)

```
test/
â”œâ”€â”€ models/prompt_tracker/          # Model tests (6 files)
â”‚   â”œâ”€â”€ prompt_test.rb
â”‚   â”œâ”€â”€ prompt_version_test.rb
â”‚   â”œâ”€â”€ llm_response_test.rb
â”‚   â”œâ”€â”€ evaluation_test.rb
â”‚   â”œâ”€â”€ ab_test_test.rb
â”‚   â””â”€â”€ prompt_file_test.rb
â”œâ”€â”€ services/prompt_tracker/        # Service tests (11 files)
â”‚   â”œâ”€â”€ file_sync_service_test.rb
â”‚   â”œâ”€â”€ llm_call_service_test.rb
â”‚   â”œâ”€â”€ cost_calculator_test.rb
â”‚   â”œâ”€â”€ evaluation_service_test.rb
â”‚   â”œâ”€â”€ evaluation_helpers_test.rb
â”‚   â”œâ”€â”€ response_extractor_test.rb
â”‚   â””â”€â”€ evaluators/                 # Evaluator tests (4 files)
â”‚       â”œâ”€â”€ format_evaluator_test.rb
â”‚       â”œâ”€â”€ keyword_evaluator_test.rb
â”‚       â”œâ”€â”€ length_evaluator_test.rb
â”‚       â””â”€â”€ llm_judge_evaluator_test.rb
â””â”€â”€ controllers/prompt_tracker/     # Controller tests (1 file)
    â””â”€â”€ basic_authentication_test.rb
```

### RSpec Tests (`spec/` directory)

```
spec/
â”œâ”€â”€ models/prompt_tracker/
â”‚   â””â”€â”€ evaluator_config_spec.rb           # 36 examples
â”œâ”€â”€ services/prompt_tracker/
â”‚   â”œâ”€â”€ ab_test_analyzer_spec.rb           # 14 examples
â”‚   â”œâ”€â”€ ab_test_coordinator_spec.rb        # 19 examples
â”‚   â”œâ”€â”€ auto_evaluation_service_spec.rb    # 10 examples
â”‚   â””â”€â”€ evaluator_registry_spec.rb         # 21 examples
â”œâ”€â”€ factories/prompt_tracker/              # FactoryBot factories
â”‚   â”œâ”€â”€ prompts.rb
â”‚   â”œâ”€â”€ prompt_versions.rb
â”‚   â”œâ”€â”€ llm_responses.rb
â”‚   â”œâ”€â”€ evaluations.rb
â”‚   â”œâ”€â”€ ab_tests.rb
â”‚   â””â”€â”€ evaluator_configs.rb
â””â”€â”€ support/                               # Test configuration
    â”œâ”€â”€ database_cleaner.rb
    â”œâ”€â”€ factory_bot.rb
    â””â”€â”€ shoulda_matchers.rb
```

---

## ğŸ¯ What Each Suite Tests

### Minitest Coverage
- âœ… **Models:** Basic CRUD, validations, associations
- âœ… **Services:** File sync, LLM calls, cost calculation, evaluators
- âœ… **Controllers:** Authentication
- âœ… **Integration:** Basic navigation

### RSpec Coverage (Comprehensive Business Logic)

#### Models & Services (~100 tests)
- âœ… **EvaluatorConfig:** Dependencies, circular detection, priority, validation
- âœ… **AutoEvaluationService:** Auto-evaluation on response creation, sync/async modes
- âœ… **AbTestCoordinator:** Variant selection, traffic splitting, randomization
- âœ… **AbTestAnalyzer:** Statistical analysis, winner determination, confidence intervals
- âœ… **EvaluatorRegistry:** Registration, lookup, building, metadata

#### Controllers (144 tests)
- âœ… **PromptsController:** CRUD operations, pagination, search
- âœ… **PromptVersionsController:** Version management, activation, responses
- âœ… **EvaluatorConfigsController:** Config CRUD, validation, dependencies
- âœ… **AbTestsController:** A/B test lifecycle, pause/resume, winner declaration
- âœ… **LlmResponsesController:** Response listing, filtering, pagination
- âœ… **EvaluationsController:** Evaluation CRUD, manual evaluations, sorting
- âœ… **Analytics::DashboardController:** Dashboard data, charts, recent activity

#### Background Jobs (27 tests)
- âœ… **EvaluationJob:** Async evaluation execution, dependency checking, error handling
- âœ… **LlmJudgeEvaluationJob:** Manual LLM judge evaluations, retry logic, metadata storage

---

## ğŸ”§ Running Specific Tests

### Minitest - Run Specific Files

```bash
# Run a specific test file
bundle exec rails test test/models/prompt_tracker/prompt_test.rb

# Run a specific test method
bundle exec rails test test/models/prompt_tracker/prompt_test.rb:10

# Run all model tests
bundle exec rails test test/models/**/*_test.rb

# Run all service tests
bundle exec rails test test/services/**/*_test.rb
```

### RSpec - Run Specific Files

```bash
# Run a specific spec file
bundle exec rspec spec/models/prompt_tracker/evaluator_config_spec.rb

# Run a specific example
bundle exec rspec spec/models/prompt_tracker/evaluator_config_spec.rb:25

# Run with documentation format
bundle exec rspec --format documentation

# Run with progress format (default)
bundle exec rspec --format progress
```

---

## ğŸ“Š Test Output

### Successful Run
```
âœ… All tests passed!
Minitest: âœ… PASSED (412 examples)
RSpec:    âœ… PASSED (100 examples)
```

### Failed Run
```
âŒ Some tests failed!
Minitest: âœ… PASSED (412 examples)
RSpec:    âŒ FAILED (95/100 passed, 5 failures)
```

---

## ğŸ› ï¸ Continuous Integration

Add to your CI pipeline (e.g., GitHub Actions):

```yaml
- name: Run all tests
  run: bundle exec rake test_all
```

Or use the script:

```yaml
- name: Run all tests
  run: ./bin/test_all
```

---

## ğŸ“ Writing New Tests

### For Minitest
```ruby
# test/models/prompt_tracker/my_model_test.rb
require "test_helper"

module PromptTracker
  class MyModelTest < ActiveSupport::TestCase
    test "should do something" do
      # Your test here
    end
  end
end
```

### For RSpec
```ruby
# spec/models/prompt_tracker/my_model_spec.rb
require "rails_helper"

RSpec.describe PromptTracker::MyModel do
  describe "#method_name" do
    it "does something" do
      # Your test here
    end
  end
end
```

---

## ğŸ“Š Test Coverage Reports

PromptTracker uses **SimpleCov** to track test coverage across both Minitest and RSpec.

### View Coverage Report

After running tests, open the HTML coverage report:

```bash
# Run all tests (generates coverage report)
bin/test_all

# Open coverage report in browser
open coverage/index.html
```

### Coverage Metrics

- **Line Coverage:** 89.64% (1842 / 2055 lines)
- **Branch Coverage:** 69.64% (539 / 774 branches)
- **Minimum Threshold:** 85% line coverage, 70% per-file coverage

### Coverage by Category

The report groups files by category:
- **Models** - Domain models and business logic
- **Controllers** - HTTP request handling
- **Services** - Business logic services
- **Jobs** - Background job processing
- **Helpers** - View helpers
- **Evaluators** - Evaluation implementations

### Understanding Coverage

- **Green files** (>90%) - Excellent coverage
- **Yellow files** (70-90%) - Good coverage
- **Red files** (<70%) - Needs more tests

**Note:** 100% coverage is not the goal. Focus on testing critical business logic and edge cases.

---

## ğŸ“ Best Practices

1. **Always run both suites** before committing
2. **Use factories** (FactoryBot) for test data in RSpec
3. **Use fixtures** for test data in Minitest
4. **Keep tests isolated** - each test should be independent
5. **Test edge cases** - not just happy paths
6. **Use descriptive test names** - explain what you're testing
7. **Disable auto-evaluation in tests** - Use `:disabled` trait on evaluator configs to prevent `after_create` callbacks from interfering

---

## ğŸ› Troubleshooting

### Database Issues
```bash
# Reset test database
RAILS_ENV=test bundle exec rails db:reset
```

### Factory Issues
```bash
# Check factory definitions
bundle exec rails console
FactoryBot.factories.map(&:name)
```

### Clear Test Logs
```bash
rm -f test/dummy/log/test.log
```

---

## ğŸ“ˆ Coverage Goals

- **Current:** ~512 tests
- **Models:** 87.5% (7/8 tested)
- **Services:** 100% (11/11 tested)
- **Controllers:** 14% (1/7 tested) - Needs improvement
- **Jobs:** 0% (0/3 tested) - Needs improvement

---

For more details, see `TESTING_PLAN.md`
