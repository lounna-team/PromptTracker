<%# Shared partial for displaying tracking code example %>
<%# Usage: render "prompt_tracker/shared/tracking_code_example", prompt: @prompt, version: @version %>

<% if local_assigns[:prompt] && local_assigns[:version] %>
  <div class="alert alert-info mb-4">
    <i class="bi bi-info-circle"></i>
    <strong>Quick Start for "<%= prompt.name %>" (v<%= version.version_number %>)</strong>
    <p class="mb-0 mt-2">Copy this code to start tracking calls for this specific prompt:</p>
  </div>

  <div class="card mb-4">
    <div class="card-body">
      <pre><code class="language-ruby"># Track a call for <%= prompt.name %> (v<%= version.version_number %>)
result = PromptTracker::LlmCallService.track(
  prompt_slug: "<%= prompt.slug %>",
  variables: {
    <%= version.variables_schema.map { |v| "#{v['name']}: \"your_value\"" }.join(",\n    ") %>
  }<% if version.model_config.present? && version.model_config["provider"] && version.model_config["model"] %>,
  # provider/model are optional - defaults to version's model_config:
  # provider: "<%= version.model_config["provider"] %>", model: "<%= version.model_config["model"] %>"
  # Uncomment above to override for testing different models<% else %>,
  provider: "openai",  # Required if not in version's model_config
  model: "gpt-4o"      # Required if not in version's model_config<% end %>,
  user_id: current_user&.id,  # Optional: track which user made the call
  environment: Rails.env.to_s  # Optional: "production", "staging", etc.
) do |rendered_prompt|
  # Make your actual LLM API call here
  # The rendered_prompt contains the template with variables filled in

  # Option 1: Return just the text (simplest)
  "Your LLM response text here"

  # Option 2: Return structured hash with token counts
  # client = OpenAI::Client.new
  # response = client.chat(
  #   parameters: {
  #     model: "gpt-4o",
  #     messages: [{ role: "user", content: rendered_prompt }]
  #   }
  # )
  # {
  #   text: response.dig("choices", 0, "message", "content"),
  #   tokens_prompt: response.dig("usage", "prompt_tokens"),
  #   tokens_completion: response.dig("usage", "completion_tokens")
  # }
end

# Use the result
puts result[:response_text]  # The LLM's response
puts result[:tracking_id]    # Unique ID for this call
</code></pre>
    </div>
  </div>
<% end %>
