<%# Configuration form for LLM Judge Evaluator %>

<div class="alert alert-info mb-3">
  <i class="bi bi-robot"></i>
  <strong>GPT-4 Judge</strong>
  <p class="mb-0 mt-1">Uses GPT-4 to evaluate response quality based on custom criteria.</p>
</div>

<!-- Judge Model -->
<div class="mb-3">
  <label class="form-label">Judge Model</label>
  <select name="config[judge_model]" class="form-select" required>
    <option value="gpt-4">GPT-4</option>
    <option value="gpt-4-turbo">GPT-4 Turbo</option>
    <option value="gpt-3.5-turbo">GPT-3.5 Turbo</option>
    <option value="claude-3-opus">Claude 3 Opus</option>
    <option value="claude-3-sonnet">Claude 3 Sonnet</option>
  </select>
  <small class="form-text text-muted">
    The LLM model to use as a judge.
  </small>
</div>

<!-- Criteria -->
<div class="mb-3">
  <label class="form-label">Evaluation Criteria</label>
  <p class="text-muted small mb-2">Select the criteria the judge will use to evaluate responses. Each criterion has a specific evaluation prompt.</p>

  <%
    criteria_options = {
      "accuracy" => "Is the response factually correct and accurate?",
      "helpfulness" => "Is the response helpful and addresses the user's needs?",
      "clarity" => "Is the response clear and easy to understand?",
      "tone" => "Is the tone appropriate and professional?",
      "professionalism" => "Does the response maintain a professional demeanor?",
      "completeness" => "Does the response fully address the question?",
      "conciseness" => "Is the response concise without unnecessary information?",
      "relevance" => "Is the response relevant to the user's question?",
      "technical_accuracy" => "Is the technical information correct and precise?"
    }
  %>

  <div class="border rounded p-3" style="max-height: 300px; overflow-y: auto;">
    <% criteria_options.each do |value, description| %>
      <div class="form-check mb-2">
        <input class="form-check-input"
               type="checkbox"
               name="config[criteria][]"
               value="<%= value %>"
               id="criteria_<%= value %>"
               <%= 'checked' if ['accuracy', 'helpfulness', 'clarity'].include?(value) %>>
        <label class="form-check-label" for="criteria_<%= value %>">
          <strong><%= value.titleize %></strong>
          <br>
          <small class="text-muted"><%= description %></small>
        </label>
      </div>
    <% end %>
  </div>
  <small class="form-text text-muted mt-2">
    <i class="bi bi-info-circle"></i> The judge LLM will be asked these specific questions for each selected criterion.
  </small>
</div>

<!-- Custom Instructions -->
<div class="mb-3">
  <label class="form-label">Custom Instructions (Optional)</label>
  <textarea name="config[custom_instructions]"
            class="form-control"
            rows="4"
            placeholder="Define what the evaluator assesses (e.g., 'Evaluate as a customer support manager' or 'Focus on technical correctness for a senior developer audience')"></textarea>
  <small class="form-text text-muted">
    Provide any additional context or specific instructions for the LLM judge.
  </small>
</div>

<!-- Score Range -->
<div class="row">
  <div class="col-md-6 mb-3">
    <label class="form-label">Minimum Score</label>
    <input type="number"
           name="config[score_min]"
           class="form-control"
           value="0"
           min="0"
           required>
  </div>
  <div class="col-md-6 mb-3">
    <label class="form-label">Maximum Score</label>
    <input type="number"
           name="config[score_max]"
           class="form-control"
           value="100"
           min="0"
           required>
  </div>
</div>

<div class="alert alert-warning">
  <i class="bi bi-exclamation-triangle"></i>
  <strong>Note:</strong> LLM judge evaluations require API calls and will run asynchronously. Make sure you have configured your API keys.
</div>
