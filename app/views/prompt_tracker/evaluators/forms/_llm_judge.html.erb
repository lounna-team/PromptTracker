<%# Form template for LLM Judge Evaluator %>
<%# This form triggers an LLM API call to automatically generate evaluation %>

<div class="alert alert-warning mb-3">
  <i class="bi bi-robot"></i>
  <strong>LLM Judge Evaluation</strong>
  <p class="mb-0 mt-1">
    This will call an LLM API to automatically evaluate the response.
    The score and feedback will be generated by the AI judge.
  </p>
</div>

<div class="row">
  <!-- Judge Model -->
  <div class="col-md-6 mb-3">
    <label for="llm_judge_model" class="form-label">Judge Model</label>
    <select name="llm_judge[judge_model]" id="llm_judge_model" class="form-select" required>
      <option vaprlue="">Select a model...</option>
      <option value="gpt-4" selected>GPT-4</option>
      <option value="gpt-4-turbo">GPT-4 Turbo</option>
      <option value="gpt-3.5-turbo">GPT-3.5 Turbo</option>
      <option value="claude-3-opus">Claude 3 Opus</option>
      <option value="claude-3-sonnet">Claude 3 Sonnet</option>
      <option value="claude-3-haiku">Claude 3 Haiku</option>
    </select>
    <small class="form-text text-muted">The LLM model that will evaluate the response</small>
  </div>

  <!-- Score Range -->
  <div class="col-md-6 mb-3">
    <label class="form-label">Score Range</label>
    <div class="row">
      <div class="col-6">
        <input type="number" name="llm_judge[score_min]" class="form-control" value="0" min="0" placeholder="Min" required>
      </div>
      <div class="col-6">
        <input type="number" name="llm_judge[score_max]" class="form-control" value="100" min="1" placeholder="Max" required>
      </div>
    </div>
    <small class="form-text text-muted">Score range for the evaluation (e.g., 0-100 or 0-5)</small>
  </div>
</div>

<!-- Evaluation Criteria -->
<div class="mb-3">
  <label class="form-label">Evaluation Criteria</label>
  <p class="text-muted small mb-2">Select the criteria the judge will use to evaluate responses. Each criterion has a specific evaluation prompt.</p>

  <%
    criteria_options = {
      "accuracy" => "Is the response factually correct and accurate?",
      "helpfulness" => "Is the response helpful and addresses the user's needs?",
      "clarity" => "Is the response clear and easy to understand?",
      "tone" => "Is the tone appropriate and professional?",
      "professionalism" => "Does the response maintain a professional demeanor?",
      "completeness" => "Does the response fully address the question?",
      "conciseness" => "Is the response concise without unnecessary information?",
      "relevance" => "Is the response relevant to the user's question?",
      "technical_accuracy" => "Is the technical information correct and precise?"
    }
  %>

  <div class="border rounded p-3" style="max-height: 300px; overflow-y: auto;">
    <% criteria_options.each do |value, description| %>
      <div class="form-check mb-2">
        <input class="form-check-input llm-judge-criterion"
               type="checkbox"
               name="llm_judge[criteria][]"
               value="<%= value %>"
               id="criterion_<%= value %>"
               <%= 'checked' if ['accuracy', 'helpfulness', 'clarity'].include?(value) %>>
        <label class="form-check-label" for="criterion_<%= value %>">
          <strong><%= value.titleize %></strong>
          <br>
          <small class="text-muted"><%= description %></small>
        </label>
      </div>
    <% end %>
  </div>
  <small class="form-text text-muted mt-2">
    <i class="bi bi-info-circle"></i> The judge LLM will be asked these specific questions for each selected criterion.
  </small>
</div>

<!-- Custom Instructions -->
<div class="mb-3">
  <label for="llm_judge_instructions" class="form-label">Custom Instructions (Optional)</label>
  <textarea name="llm_judge[custom_instructions]"
            id="llm_judge_instructions"
            class="form-control"
            rows="3"
            placeholder="Define what the evaluator assesses (e.g., 'Evaluate as a customer support manager' or 'Focus on technical correctness for a senior developer audience')"></textarea>
  <small class="form-text text-muted">Additional context or instructions for the judge LLM</small>
</div>

<!-- Preview of what will happen -->
<div class="alert alert-light border">
  <h6 class="mb-2"><i class="bi bi-info-circle"></i> What happens next:</h6>
  <ol class="mb-0 ps-3">
    <li>A background job will be created to call the LLM API</li>
    <li>The judge LLM will analyze the response based on your criteria</li>
    <li>The judge will provide:
      <ul>
        <li>Overall score (within your specified range)</li>
        <li>Individual scores for each criterion</li>
        <li>Detailed feedback explaining the scores</li>
      </ul>
    </li>
    <li>The evaluation will appear on this page once complete (usually 5-30 seconds)</li>
  </ol>
</div>

<!-- Hidden field to indicate this is an LLM judge evaluation -->
<input type="hidden" name="evaluation[evaluator_type]" value="llm_judge">

<script>
  // Add visual feedback for criteria selection
  (function() {
    const criteriaSelect = document.getElementById('llm_judge_criteria');

    if (criteriaSelect) {
      criteriaSelect.addEventListener('change', function() {
        const selectedCount = this.selectedOptions.length;
        const helpText = this.nextElementSibling;

        if (selectedCount === 0) {
          helpText.classList.add('text-danger');
          helpText.innerHTML = '<i class="bi bi-exclamation-triangle"></i> Please select at least one criterion';
        } else {
          helpText.classList.remove('text-danger');
          helpText.innerHTML = `
            Hold <kbd>Cmd</kbd> (Mac) or <kbd>Ctrl</kbd> (Windows) to select multiple criteria.
            <strong>${selectedCount} criteria selected</strong>
          `;
        }
      });
    }
  })();
</script>
