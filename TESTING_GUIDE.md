# Testing Guide: Mock vs Real LLM Mode

## Quick Start

### Test in Mock Mode (Default)

1. **Check your `.env` file:**
```bash
PROMPT_TRACKER_USE_REAL_LLM=false
```

2. **Restart your Rails server:**
```bash
# Stop the server (Ctrl+C)
# Start it again
rails s
```

3. **Run a test from the UI:**
   - Navigate to a prompt test
   - Click "Run Test"
   - You should see a mock response like:
     ```
     This is a mock response to: [your prompt]...
     In a production environment, this would be replaced with an actual API call to openai.
     ```

4. **Check the test results:**
   - Evaluators will show mock scores (80-95% range)
   - LLM judge evaluations will show mock feedback

### Test in Real Mode (OpenAI)

1. **Update your `.env` file:**
```bash
PROMPT_TRACKER_USE_REAL_LLM=true
OPENAI_API_KEY=your_openai_api_key_here
```

2. **Restart your Rails server:**
```bash
# Stop the server (Ctrl+C)
# Start it again
rails s
```

3. **Run a test from the UI:**
   - Navigate to a prompt test
   - Click "Run Test"
   - **This will make a real API call to OpenAI!**
   - You should see a real GPT-4 response

4. **Check the test results:**
   - Response will be from actual GPT-4
   - LLM judge evaluations will use real GPT-4 to evaluate
   - Costs will be incurred based on OpenAI pricing

## What to Expect

### Mock Mode Behavior

**LLM Responses:**
```
This is a mock response to: Hi {{customer_name}}! Thanks for contacting us...

In a production environment, this would be replaced with an actual API call to openai.
The response would be generated by the configured model and would address the prompt appropriately.
```

**LLM Judge Evaluations:**
```
EVALUATION RESULTS:

accuracy: 85.0/100
tone: 90.5/100
clarity: 88.2/100

Overall Score: 87.9/100

Feedback: This is a mock evaluation for testing purposes...
```

### Real Mode Behavior

**LLM Responses:**
- Actual GPT-4 generated text
- Contextually relevant to your prompt
- Natural language responses

**LLM Judge Evaluations:**
- Real GPT-4 evaluation of the response
- Detailed feedback based on criteria
- Accurate scores reflecting actual quality

## Testing Different Scenarios

### Scenario 1: Test with Different Models

**Mock Mode:**
```json
{
  "provider": "openai",
  "model": "gpt-3.5-turbo",
  "temperature": 0.7
}
```
- Will generate mock response
- Model config is ignored in mock mode

**Real Mode:**
```json
{
  "provider": "openai",
  "model": "gpt-4",
  "temperature": 0.7
}
```
- Will call actual GPT-4
- Respects temperature and other parameters

### Scenario 2: Test with LLM Judge

**Test Configuration:**
```json
{
  "evaluator_key": "gpt4_judge",
  "threshold": 85,
  "config": {
    "judge_model": "gpt-4",
    "criteria": ["accuracy", "tone", "clarity"],
    "score_max": 100
  }
}
```

**Mock Mode:**
- Generates random scores (80-95% of max)
- Returns structured mock evaluation

**Real Mode:**
- Calls GPT-4 to evaluate the response
- Returns real evaluation with detailed feedback

### Scenario 3: Error Handling

**Missing API Key (Real Mode):**
```bash
# .env
PROMPT_TRACKER_USE_REAL_LLM=true
# OPENAI_API_KEY not set
```

**Expected Result:**
```
API key missing: OPENAI_API_KEY environment variable not set.
Please configure your API keys in .env file.
```

**Invalid API Key (Real Mode):**
```bash
OPENAI_API_KEY=invalid-key
```

**Expected Result:**
```
LLM API error: OpenAI API error: [error message from OpenAI]
```

## Verification Checklist

### Mock Mode ✓
- [ ] Test runs successfully
- [ ] Response contains "This is a mock response"
- [ ] No API calls made (check logs)
- [ ] Evaluators return mock scores
- [ ] Fast execution (< 1 second)

### Real Mode ✓
- [ ] Test runs successfully
- [ ] Response is from actual LLM
- [ ] API call logged in Rails logs
- [ ] Evaluators return real scores
- [ ] Slower execution (depends on API)
- [ ] Costs incurred (check OpenAI dashboard)

## Troubleshooting

### Issue: Still getting mock responses in real mode

**Solution:**
1. Check `.env` file: `PROMPT_TRACKER_USE_REAL_LLM=true`
2. Restart Rails server
3. Check Rails logs for "Using real LLM API" message

### Issue: API key error

**Solution:**
1. Verify API key is correct in `.env`
2. Check API key has not expired
3. Verify API key has sufficient credits

### Issue: Tests failing with real mode

**Solution:**
1. Check OpenAI API status
2. Verify model name is correct (e.g., "gpt-4" not "gpt4")
3. Check rate limits on your OpenAI account

## Cost Monitoring

### Estimated Costs (OpenAI)

**GPT-4:**
- Prompt: ~$0.03 per 1K tokens
- Completion: ~$0.06 per 1K tokens
- Average test: ~$0.01 - $0.05

**GPT-3.5-turbo:**
- Prompt: ~$0.0015 per 1K tokens
- Completion: ~$0.002 per 1K tokens
- Average test: ~$0.001 - $0.005

**Tips:**
- Use mock mode for development
- Use gpt-3.5-turbo for initial testing
- Only use gpt-4 for final validation
- Set `max_tokens` to limit costs

## Next Steps

1. **Test in mock mode** to verify everything works
2. **Enable real mode** for one test to verify API integration
3. **Monitor costs** on OpenAI dashboard
4. **Switch back to mock mode** for regular development
5. **Use real mode** only when validating production prompts

